{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LANGUAGE MODEL\n",
        "*VietAI Advanced NLP*"
      ],
      "metadata": {
        "id": "DBByBhMGwJvy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt4n1G1kIQqQ"
      },
      "source": [
        "# Addition with LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abdhWXKiIQqZ"
      },
      "source": [
        "Inspired from Andrej Karpathy's [play_math](https://github.com/karpathy/minGPT/blob/master/play_math.ipynb), in this tutorial, you will learn how to calculate addition using LSTM !!!\n",
        "\n",
        "You will learn:\n",
        "1. Create custom dataset\n",
        "2. Modeling input, output for a well-structured dataset\n",
        "3. Masking the labels, so that model only learn on specific part of the labels.\n",
        "4. Build a LSTM model, train and evaluate the trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6XLLfkpIQqa"
      },
      "source": [
        "## 1. Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> The sum of two n-digit numbers gives a third up to (n+1)-digit number. So our\n",
        "    encoding will simply be the n-digit first number, n-digit second number, \n",
        "    and (n+1)-digit result, **all simply concatenated together**. Because each addition\n",
        "    problem is so structured, there is no need to bother the model with encoding\n",
        "    +, =, or other tokens. Each possible sequence has the **same length**, and simply\n",
        "    contains the raw digits of the addition problem. - [From Andrej Karapthy's play_math](https://github.com/karpathy/minGPT/blob/master/play_math.ipynb)\n",
        "    \n",
        "\n",
        "Examples:\n",
        "  - ```85 + 50 = 135``` becomes the sequence ```[8, 5, 5, 0, 1, 3, 5]```\n",
        "  - ```47 + 17 =  64``` becomes the sequence ```[4, 7, 1, 7, 0, 6, 4]```\n",
        "  \n",
        "  etc."
      ],
      "metadata": {
        "id": "ATwXH8DtsyTR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW49lUV6ti_w"
      },
      "source": [
        "Example of the 2-digit problems:\n",
        "\n",
        "    - 47 + 17 =  64 becomes the sequence [4, 7, 1, 7, 0, 6, 4]\n",
        "    \n",
        "\n",
        "\n",
        "We will also only train LSTM on the final (n+1)-digits because the first\n",
        "two n-digits are always assumed to be given. So when we give LSTM an exam later,\n",
        "we will e.g. feed it the sequence ```[4, 7, 1, 7]```, which encodes that we'd like to add ```47 + 17```, and hope that the model completes the integer sequence with ```[0, 6, 4]``` in 3 sequential steps.\n",
        "\n",
        "---\n",
        "Example of an item that was generated by this dataset:\n",
        "\n",
        "input, target = train_dataset[0]\n",
        "> input: ```tensor([4, 7, 1, 7, 0, 6])```\n",
        "\n",
        "> target: ```tensor([-100, -100, -100,    0,    6,    4])```\n",
        "    \n",
        "---\n",
        "\n",
        "Explaination:\n",
        "\n",
        "1. equation: ```47 + 17 = 064```\n",
        "2. concat all digits together: ```[4, 7, 1, 7, 0, 6, 4]```\n",
        "3. prepare data for language model teacher-forcing objective (predict next digits)\n",
        "\n",
        "```\n",
        "    target:  7     1     7     0     6     4  \n",
        "             |     |     |     |     |     |\n",
        "    input :  4     7     1     7     0     6\n",
        "```\n",
        "\n",
        "4. Since the model just need to learn to yield the summation, we could ignore the loss from the given number to make the learning progress more efficient, by mask the target with specific index (e.g -100)\n",
        "\n",
        "```\n",
        "    contribute to loss:   âœ–    âœ–     âœ–     âœ”     âœ”     âœ” \n",
        "               target : -100  -100  -100    0     6     4\n",
        "                          |     |     |     |     |     |\n",
        "               input  :   4     7     1     7     0     6\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Define the dataset"
      ],
      "metadata": {
        "id": "jCaH9DSEr3fb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAYBZq0Zn9aU"
      },
      "outputs": [],
      "source": [
        "# !pip install torch tqdm numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRb4TyHHIQqb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djLm_Z4hIQqc"
      },
      "outputs": [],
      "source": [
        "class AdditionDataset(Dataset):\n",
        "\n",
        "    def __init__(self, ndigit, split):\n",
        "        \"\"\"\n",
        "          ndigit: number of digits \n",
        "          split: train/ test\n",
        "        \"\"\"\n",
        "        self.split = split # train/test\n",
        "        self.ndigit = ndigit\n",
        "        self.vocab_size = 10 # 10 possible digits 0..9\n",
        "        \n",
        "        # split up all addition problems into either training data or test data\n",
        "        num = (10**self.ndigit)**2 # total number of possible combinations\n",
        "        num_test = min(int(num*0.2), 1000) # 20% of the whole dataset, or only up to 1000\n",
        "        \n",
        "        r = np.random.RandomState(1337) # make deterministic\n",
        "        perm = r.permutation(num)\n",
        "        self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.ixes.size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # given a problem index idx, first recover the associated a + b\n",
        "        idx = self.ixes[idx]\n",
        "        nd = 10**self.ndigit\n",
        "        a = idx // nd\n",
        "        b = idx %  nd\n",
        "        c = a + b\n",
        "        render = f'%0{self.ndigit}d%0{self.ndigit}d%0{self.ndigit+1}d' % (a,b,c) # e.g. 03+25=28 becomes \"0325028\" \n",
        "        dix = [int(s) for s in render] # convert each character to its token index\n",
        "        # x will be input to LSTM and y will be the associated expected outputs\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n",
        "        \n",
        "        y[:self.ndigit*2-1] = -100 # we will only train in the output locations. -100 will mask loss to zero\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZ8z0-HXIQqe"
      },
      "outputs": [],
      "source": [
        "# create a dataset for e.g. 2-digit addition\n",
        "ndigit = 2\n",
        "train_dataset = AdditionDataset(ndigit= ndigit, split= 'train')\n",
        "test_dataset = AdditionDataset(ndigit= ndigit, split= 'test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFEAquLuIQqf"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQb3j6bpIQqi"
      },
      "source": [
        "## 3. Define the model\n",
        "\n",
        "\n",
        "We will implement a neural network with two LSTM layers, vocab_size = 10 (10 digits), 1024 hidden state, with p_dropout = 0.2 between each layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YClhCsweZ6L"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrnXQMdQIQqj"
      },
      "outputs": [],
      "source": [
        "n_layers = 2\n",
        "vocab_size = 10\n",
        "hidden_size = 1024\n",
        "p_dropout = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kh0P8b7IQqk"
      },
      "outputs": [],
      "source": [
        "class AdditionLSTM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AdditionLSTM, self).__init__()\n",
        "        \"\"\"\n",
        "        layers:\n",
        "          1. Embedding\n",
        "          2. LSTM (with n_layers layers, droprate is p_dropout, batch size is first demension)\n",
        "          3. Linear\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE ###\n",
        "        # using nn module in pytorch\n",
        "        self.layer1 = \n",
        "        self.layer2 = \n",
        "        self.layer3 =\n",
        "        \n",
        "    def forward(self, word_seq):\n",
        "        g_seq                      =   self.layer1( word_seq )\n",
        "        h_seq , (h_final,c_final)  =   self.layer2(g_seq)      \n",
        "        score_seq                  =   self.layer3( h_seq )\n",
        "        return score_seq,  h_final , c_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LIM_K-cIQqk"
      },
      "outputs": [],
      "source": [
        "net = AdditionLSTM()\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Lnu6uYfIQql"
      },
      "source": [
        "## 4. Implement the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZp8wUktHxpi"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "seed = 0\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRq-jzghIQqm"
      },
      "outputs": [],
      "source": [
        "# Some config for the training\n",
        "n_epochs = 500\n",
        "batch_size = 512\n",
        "lr = 0.002\n",
        "device = torch.device(\"cuda\")\n",
        "log_every = 10\n",
        "\n",
        "net = net.to(device)\n",
        "optimizer = optim.AdamW(net.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_epoch(split):\n",
        "    is_train = split == \"train\"\n",
        "    net.train(is_train) # set to train False when eval\n",
        "    data = train_dataset if is_train else test_dataset\n",
        "    loader = DataLoader(data, batch_size=batch_size, shuffle=True, num_workers=2, worker_init_fn=np.random.seed(seed))\n",
        "\n",
        "    losses = []\n",
        "    \n",
        "    for input, target in loader:\n",
        "        # send mini batch to device\n",
        "        input = input.to(device)\n",
        "        target = target.to(device)\n",
        "    \n",
        "        # forward the model\n",
        "        with torch.set_grad_enabled(is_train):\n",
        "            scores, h, c = net(input) # scores: batch_size, seq_len, vocab_size\n",
        "\n",
        "            # reshape the scores and target to huge batch of size bs*seq_length\n",
        "            scores = scores.view(-1, vocab_size)\n",
        "            target = target.flatten()\n",
        "            loss = criterion(scores, target)\n",
        "            losses.append(loss.item())\n",
        "        \n",
        "        if is_train:\n",
        "            net.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        return float(np.mean(losses))\n",
        "        \n",
        "        \n",
        "for epoch in range(n_epochs):\n",
        "    train_loss = run_epoch('train')\n",
        "    test_loss = run_epoch('test')\n",
        "    if (epoch + 1) % log_every == 0:\n",
        "        print(f\"Epoch: {epoch + 1:3d} Train loss: {train_loss:.5f} Test loss: {test_loss:.5f}\")"
      ],
      "metadata": {
        "id": "5nzN1smJ6kTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMH5KtY_IQqn"
      },
      "source": [
        "## 5. Define the sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will define the way we sample the model's output to get the final prediction.\n",
        "\n",
        "Normally, we take the index with the highest score to predict. However, in this section, we will define a more general sampling function based on k highest scores.\n",
        "\n",
        "In some situations, we may want not to take the prediction with the highest scores but sample from top k scores or we may want to keep the k best candidates (eg. beam search). These functions will help you do that.\n",
        "\n",
        "```\n",
        "1. top_k_logits: remain the top k elements and mask the others to -INFINITY\n",
        "2. generate_top_k: generate the prediction \n",
        "  * top_k (int): prediction only based on the top k logits\n",
        "  * sample (True): sample random from top k score (just naive sampling ðŸ˜†)\n",
        "  or simply take the index with highest score \n",
        "```\n",
        "\n",
        "*Our evaluation simply uses the highest score to generate prediction*"
      ],
      "metadata": {
        "id": "A0YJUwreVwrc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8TGZoG4IQqn"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def top_k_logits(logits, k):\n",
        "    \"\"\"\n",
        "    Remain top k logits and mask the others to -INFINITY\n",
        "\n",
        "    logits: input logits matrix (size [B,length,vocab_size])\n",
        "    k: the number of remaining elements\n",
        "\n",
        "    examples:\n",
        "     a = [[1.2, 5.0, -3.4, 4.2, -2.1],\n",
        "          [2.1, -0.4, -3.0, 1.7, 0.1]]\n",
        "\n",
        "     => top_k_logits(a, k = 2) = [[-INF, 5.0, -INF, 4.2, -INF],\n",
        "                                  [2.1, -INF, -INF, 1.7, -INF]]\n",
        "    \"\"\"\n",
        "\n",
        "    # hint: torch.topk() might be useful\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def generate_top_k(ids, n_char=200, sample=False, top_k=None):\n",
        "    \n",
        "    \"\"\"\n",
        "    ids: input ids\n",
        "    n_char: number of steps to generate final output (n-digit corresponds to n-digit + 1 steps)\n",
        "    sample: sample random from top k score\n",
        "    top_k: get top_k prediction score\n",
        "    \"\"\"\n",
        "    \n",
        "    prompt_ids = ids[:] # copy\n",
        "    with torch.no_grad():\n",
        "        for _ in range(n_char):\n",
        "            input = torch.LongTensor(prompt_ids).reshape(1, -1).to(device)\n",
        "            scores, h, c = net(input)\n",
        "            # scores shape (B, L, V), B = 1\n",
        "            logits = scores[:, -1, :]\n",
        "\n",
        "            # apply softmax to convert to probabilities\n",
        "            if top_k is not None:\n",
        "                logits = top_k_logits(logits, top_k)\n",
        "            \n",
        "            \n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "            if sample:\n",
        "                ix = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "                # ix = np.argmax(probs.cpu().numpy(), axis= -1)\n",
        "\n",
        "              \n",
        "\n",
        "            prompt_ids.append(ix.item())\n",
        "    return prompt_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9B7TFSkIQqo"
      },
      "outputs": [],
      "source": [
        "net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "to5kDAB6IQqo"
      },
      "outputs": [],
      "source": [
        "generate_top_k([1, 2, 3, 5], n_char=3) # We expect [1, 2, 3, 5, 0, 4, 7] since 12 + 35 = 047"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvSB4g_QIQqn"
      },
      "outputs": [],
      "source": [
        "def to_integer(list_digit):\n",
        "    \"\"\"\n",
        "    Convert a list of digits to number\n",
        "    e.g [1, 2] => 12\n",
        "    \"\"\"\n",
        "    out = 0\n",
        "    reverse_list_digit = list_digit[::-1]\n",
        "    for factor, i in enumerate(reverse_list_digit):\n",
        "        out += 10**factor*i\n",
        "    return out\n",
        "\n",
        "to_integer([1, 2, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUSzIGIHIQqo"
      },
      "outputs": [],
      "source": [
        "# now let's evaluate our trained model\n",
        "def evaluate(dataset):\n",
        "    \n",
        "    results = []\n",
        "    for x, y in tqdm(dataset):\n",
        "        d1d2 = x[:ndigit*2].numpy().tolist() # Take first two term as prompt\n",
        "        d1d2d3 = generate_top_k(d1d2, ndigit+1)\n",
        "        d3 = d1d2d3[-(ndigit+1):] # Take the last ndigit+1\n",
        "        \n",
        "        # decode the integers from individual digits\n",
        "        d1i = to_integer(d1d2[:ndigit])\n",
        "        d2i = to_integer(d1d2[ndigit:])\n",
        "        \n",
        "        d3i_pred = to_integer(d3)\n",
        "        d3i_gt = d1i + d2i\n",
        "        correct = (d3i_pred == d3i_gt)\n",
        "        results.append(int(correct))\n",
        "        \n",
        "        judge = 'YEP!!!' if correct else 'NOPE'\n",
        "        if not correct:\n",
        "            print(\"LSTM claims that %03d + %03d = %03d (gt is %03d; %s)\" \n",
        "                % (d1i, d2i, d3i_pred, d3i_gt, judge))\n",
        "        \n",
        "    print(\"final score: %d/%d = %.2f%% correct\" % (np.sum(results), len(results), 100*np.mean(results)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cArso3LOIQqo"
      },
      "outputs": [],
      "source": [
        "# training set: how well did we memorize?\n",
        "evaluate(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLmeWsugIQqp"
      },
      "outputs": [],
      "source": [
        "# test set: how well did we generalize?\n",
        "evaluate(test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab_lstm.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}