{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[Exercise] Multihead self-attention.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"cqqZGZwhFyAe"},"source":["# Multihead Self-Attention\n","\n","*VietAI Advanced NLP*"]},{"cell_type":"markdown","metadata":{"id":"PeDTfnHdFyAf"},"source":["In this exercise, we will build Multihead Self-Attention using Pytorch, fully parallelized with multiple queries, multiple heads and multiple sequences in a batch (using broadcasting). This component will then be used to build the Transformers architecture.\n","\n","Notes:\n","- Lowercase characters (e.g., `q`, `k`, `v`) represent 1 vector / 1-dimensional tensor.\n","- Uppercase characters (e.g. `Q`, `K`, `V`) represent 2 or more dimensional tensor (The number of dimensions will be in the comments of each function).\n","- You need to complete the sections in the mark as follows:\n","\n","```python\n","########### YOUR CODE HERE #################\n","###########################################\n","```\n","\n","We start by installing the necessary libraries & some auxiliary functions:"]},{"cell_type":"code","metadata":{"id":"7WnXQIAkFyAh","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cd8f2e56-c526-4c0d-88ab-f150c3f3daf1"},"source":["!pip install einops\n","!wget -c https://gist.githubusercontent.com/Luvata/55f7b3e9ae451122b9e3faf0a7387b4f/raw/440fac5c6e7153fd39e4eb9ebec6e51c9520ef1f/visualize.py\n","!pip install --upgrade graphviz"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting einops\n","  Downloading https://files.pythonhosted.org/packages/5d/a0/9935e030634bf60ecd572c775f64ace82ceddf2f504a5fd3902438f07090/einops-0.3.0-py2.py3-none-any.whl\n","Installing collected packages: einops\n","Successfully installed einops-0.3.0\n","--2021-02-24 15:22:25--  https://gist.githubusercontent.com/Luvata/55f7b3e9ae451122b9e3faf0a7387b4f/raw/440fac5c6e7153fd39e4eb9ebec6e51c9520ef1f/visualize.py\n","Resolving gist.githubusercontent.com (gist.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to gist.githubusercontent.com (gist.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 9008 (8.8K) [text/plain]\n","Saving to: ‘visualize.py’\n","\n","visualize.py        100%[===================>]   8.80K  --.-KB/s    in 0s      \n","\n","2021-02-24 15:22:26 (39.9 MB/s) - ‘visualize.py’ saved [9008/9008]\n","\n","Collecting graphviz\n","  Downloading https://files.pythonhosted.org/packages/86/86/89ba50ba65928001d3161f23bfa03945ed18ea13a1d1d44a772ff1fa4e7a/graphviz-0.16-py2.py3-none-any.whl\n","Installing collected packages: graphviz\n","  Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.16\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"o6bJQC64FyAh"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from einops import rearrange\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from visualize import display_module"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Les8Ji48FyAi"},"source":["def benchmark(input_generator, slow_function, fast_function, n_tries=100):\n","    \"\"\"\n","    This function will verify that the result of `slow_function` and\n","    `fast_function` are equals for all `n_tries` inputs from `input_generator`\n","    It also prints out the average run time of each function\n","    \"\"\"\n","    def _one_try(input, function):\n","        start_time = time.time()\n","        output = function(*input)\n","        return time.time() - start_time, output\n","    \n","    def _stat_str(list_times):\n","        return f\"AVG: {np.mean(list_times)}+-{np.std(list_times)}\"\n","    \n","    slow_times = []\n","    fast_times = []\n","    for i in range(n_tries):\n","        input = next(input_generator)\n","        slow_time, slow_result = _one_try(input, slow_function)\n","        fast_time, fast_result = _one_try(input, fast_function)\n","        assert torch.allclose(slow_result, fast_result)\n","        slow_times.append(slow_time)\n","        fast_times.append(fast_time)\n","        \n","    print(\"Your output is correct\")\n","    print(\"Timing of slow function: \", _stat_str(slow_times))  \n","    print(\"Timing of fast function: \", _stat_str(fast_times))\n","    print(f\"Speedup: {np.mean(slow_times) / np.mean(fast_times)} times\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n7YaRGvbFyAj"},"source":["$d_k$, $d_v$, $n_{head}$, .. are the hyperparameters in Transformers. N_queries, M_keys are configurations to test attention. `batch_size` is configuration to test broadcasting with input of multiple samples in a forward."]},{"cell_type":"code","metadata":{"id":"VWAqVYfNFyAk"},"source":["d_k = 64\n","d_v = 64\n","N_queries = 32\n","M_keys = 32\n","n_head = 8\n","batch_size = 32"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ma09EczFyAm"},"source":["The dot-product attention scale is calculated as follows:\n","$$Attention(q, K, V) = Softmax(\\frac{K^Tq}{\\sqrt{d_k}}) = \\sum_{i}{}\\frac{e^{score(q,k_i)}}{ \\sum_j e^{score(q, k_j)} }v_i$$\n","\n","with $$score(q, k) = \\frac{q \\cdot k}{\\sqrt{d_k}}$$\n","\n","If $score(q, k_i) = -\\infty \\rightarrow e^{score(q, k_i)} = e^{-\\infty} = 0 $ then the weight of $v_i$ will be zero so $v_i$ will not contribute information to the attention result, in other words, $v_i$ has been `mask` when calculating attention.\n","\n","```\n","sequence              <BOS>     I    go    to    school  <EOS> <PAD> <PAD>\n","                        |       |     |    |       |       |     |     |\n","                        v       v     v    v       v       v     v     v\n","PAD mask                0       0     0    0       0       0     1     1 \n","```\n","\n","In the LSTM seq2seq + Attention architecture (Bahdanau et al., Luong et al.,), the hidden state's Attention at 1 decode step calculated on the embedding of $M$ tokens encoder, will be used `mask` with `PAD` tokens in the input sentence, because the token `PAD` does not carry the information of the sentence but only to normalize the equal length of strings.\n","\n","In Attention, the use of `mask` is necessary when we need to remove the contribution of (k,v) that carries no information (e.g., `PAD` token) or has no link to the current query (e.g. `causal- mask` in the Transformer decoder).\n","\n","In Pytorch, the `mask` on a tensor is done via the `masked_fill` function as follows:"]},{"cell_type":"code","metadata":{"id":"0Crhu7NcFyAn","colab":{"base_uri":"https://localhost:8080/"},"outputId":"695d77b4-ab80-4582-c908-3c44384dee40"},"source":["print(\"Original:\")\n","similar_score = torch.tensor([[0.5, 2.3, 1.4, -1.3, 3.1]])\n","print(\"similar score  :\", similar_score)\n","print(\"softmax:\", F.softmax(similar_score, dim=-1))\n","\n","n_step = similar_score.shape[1]\n","mask = torch.ones_like(similar_score).bool() # [[True, True, True, True, True]]\n","\n","for step_idx in range(n_step):\n","    mask[0, step_idx] = False\n","    masked_score = similar_score.masked_fill(mask, value=-1e9)\n","    print(f\"step #{step_idx}:\", \"mask:\", mask, \"softmax out: \", F.softmax(masked_score, dim=-1))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Original:\n","similar score  : tensor([[ 0.5000,  2.3000,  1.4000, -1.3000,  3.1000]])\n","softmax: tensor([[0.0432, 0.2615, 0.1063, 0.0071, 0.5819]])\n","step #0: mask: tensor([[False,  True,  True,  True,  True]]) softmax out:  tensor([[1., 0., 0., 0., 0.]])\n","step #1: mask: tensor([[False, False,  True,  True,  True]]) softmax out:  tensor([[0.1419, 0.8581, 0.0000, 0.0000, 0.0000]])\n","step #2: mask: tensor([[False, False, False,  True,  True]]) softmax out:  tensor([[0.1052, 0.6362, 0.2587, 0.0000, 0.0000]])\n","step #3: mask: tensor([[False, False, False, False,  True]]) softmax out:  tensor([[0.1034, 0.6253, 0.2542, 0.0171, 0.0000]])\n","step #4: mask: tensor([[False, False, False, False, False]]) softmax out:  tensor([[0.0432, 0.2615, 0.1063, 0.0071, 0.5819]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ax0eXF9_FyAo"},"source":["At step #4, the Softmax output is equal to the Softmax result without the mask. For the rest of the steps, when the keys at position $i$ are masked (`mask[0][i] == True`), the Softmax result is equivalent to that position equal to `0`. A Scale-dot Attention function using a mask in pytorch can be defined as follows:\n","\n","![scale-dot-product attention](https://raw.githubusercontent.com/Luvata/gifs/main/figures/scale_dot_product.png)"]},{"cell_type":"code","metadata":{"id":"uzYWgvHJFyAp"},"source":["def scale_dot_product_attention(q, K, V, mask):\n","    \"\"\"Scale-dot attentionn on a single query\n","    Arguments:\n","        q: torch.Tensor shape (1, d_k)\n","        K: torch.Tensor shape (M, d_k)\n","        V: torch.Tensor shape (M, d_v)\n","        mask: torch.BoolTensor shape (1, M)\n","        \n","        if mask[0, i] == True, (k, q) at index `i` will be masked\n","        when calculating attention\n","    Return:\n","        scaled-dot attention: torch.Tensor shape (1, d_v)\n","    \"\"\"\n","    _, d_k = q.shape\n","    scale = d_k ** -0.5\n","    similar_score = q @ K.T * scale # (1, d_k) @ (d_k, M) -> (1, M)\n","    similar_score = similar_score.masked_fill(mask, value=float(\"-inf\"))\n","    attention_weight = F.softmax(similar_score, dim=-1) # (1, M)\n","    attention = attention_weight @ V # (1, M) @ (M, d_v) -> (1, d_v)\n","    return attention"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cf3Kq36mUhFT"},"source":["\n","\n","- `@` is `dot` product in Pytorch\n","- Line 18: `dim=-1` is the softmax on the last axis (`M`)"]},{"cell_type":"code","metadata":{"id":"peYbACE9FyAq","colab":{"base_uri":"https://localhost:8080/"},"outputId":"0a6aa9f5-6637-468d-ee63-33eb11d08b35"},"source":["q = torch.rand(1, d_k)\n","K = torch.rand(M_keys, d_k)\n","V = torch.rand(M_keys, d_v)\n","mask = torch.randint(low=0, high=2, size=(1, M_keys)).bool()\n","\n","print(mask)\n","print(scale_dot_product_attention(q, K, V, mask).shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([[False, False, False,  True, False,  True, False,  True,  True,  True,\n","          True,  True, False, False,  True, False,  True,  True, False,  True,\n","          True,  True, False, False, False, False, False, False,  True,  True,\n","          True,  True]])\n","torch.Size([1, 64])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZXPPSuoyFyAr"},"source":["## 1. Broadcasting with multiple queries\n","\n","*Let's get started!*\n","\n","The above `scale_dot_product_attention` function only works with a query. In this section you need to complete the `queries_attention` function in parallel (using broadcasting) with `N` queries. The output should be the same as `slow_queries_attention`. Note in this section, $mask \\in \\mathbb{R}^{N \\times M}$ given `mask[i]` being the vector mask of `Q[i]` to `M` keys."]},{"cell_type":"code","metadata":{"id":"3PexeABsFyAr"},"source":["def slow_queries_attention(Q, K, V, mask):\n","    \"\"\"Attentionn on many queries\n","    Arguments:\n","        Q: torch.Tensor shape (N, d_k)\n","        K: torch.Tensor shape (M, d_k)\n","        V: torch.Tensor shape (M, d_v)\n","        mask: torch.BoolTensor shape (N, M)\n","        mask[i, j] = True means K[j] was masked for Q[i]\n","\n","    Return:\n","        scaled-dot attention: torch.Tensor shape (N, d_v)\n","    \"\"\"\n","    attentions = []\n","    for query, single_mask in zip(Q, mask):\n","        query_vector = query.unsqueeze(0)  # (d_k) -> (1, d_k)\n","        mask_vector = single_mask.unsqueeze(0) # (M) -> (1, M)\n","        attentions.append(scale_dot_product_attention(query_vector, K, V, mask_vector))\n","    attentions = torch.stack(attentions)  # (N, 1, d_v)\n","    attentions = attentions.squeeze(1)  # (N, d_v)\n","    return attentions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WrBQdOqlFyAs"},"source":["def queries_attention(Q, K, V, mask):\n","    \"\"\"Attentionn on many queries\n","    Arguments:\n","        Q: torch.Tensor shape (N, d_k)\n","        K: torch.Tensor shape (M, d_k)\n","        V: torch.Tensor shape (M, d_v)\n","        mask: torch.BoolTensor shape (N, M)\n","        mask[i, j] = True means K[j] was masked for Q[i]\n","\n","    Return:\n","        scaled-dot attention: torch.Tensor shape (N, d_v)\n","    \"\"\"\n","    N, d_k = Q.shape\n","    ########### YOUR CODE HERE #################\n","    \n","    ###########################################\n","    return attentions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gyBs_KDlFyAu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e34ab276-6bfa-48c1-963e-ae05aba51fc3"},"source":["## Test\n","Q = torch.rand(N_queries, d_k)\n","K = torch.rand(M_keys, d_k)\n","V = torch.rand(M_keys, d_v)\n","mask = torch.randint(low=0, high=2, size=(N_queries, M_keys)).bool()\n","\n","slow_attn =  slow_queries_attention(Q, K, V, mask)\n","parl_attn =  queries_attention(Q, K, V, mask)\n","\n","assert parl_attn.shape == slow_attn.shape\n","assert torch.allclose(parl_attn, slow_attn)\n","\n","def a1_generator():\n","    while True:\n","        Q = torch.rand(N_queries, d_k)\n","        K = torch.rand(M_keys, d_k)\n","        V = torch.rand(M_keys, d_v)\n","        mask = torch.randint(low=0, high=2, size=(N_queries, M_keys))\n","        yield (Q, K, V, mask)\n","        \n","generator = a1_generator()\n","benchmark(generator, slow_queries_attention, queries_attention)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Your outputs is correct\n","Timing of slow function:  AVG: 0.001767120361328125+-0.0003138023833246862\n","Timing of fast function:  AVG: 7.760763168334961e-05+-1.664969291348885e-05\n","Speedup: 22.769930263279164 times\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T8UovM77FyAv"},"source":["## 2. Broadcasting with multi-heads (Multi-head attention)\n","![heads](https://raw.githubusercontent.com/Luvata/gifs/main/figures/transformer_heads.png)\n","\n","Similar to the previous exercise, Multi-head Attention will still calculate the Scale-dot Attention of `N` queries and `M` keys but in parallel on multiple `head`.\n","\n","You need to complete the `heads_attention` function using broadcasting and the result should be the same as the `slow_heads_attention` function.\n","\n","Suggestions:\n","   - You can use `transpose` to \"shift the axis\" of a tensor.\n","   - `mask` will be applied the same for all heads."]},{"cell_type":"code","metadata":{"id":"Zzkm5IhmFyAx"},"source":["def slow_heads_attention(Q, K, V, mask):\n","    \"\"\"Slow Attentionn on many queries and many heads\n","    Arguments:\n","        Q: torch.Tensor shape (N, n_head, d_k)\n","        K: torch.Tensor shape (M, n_head, d_k)\n","        V: torch.Tensor shape (M, n_head, d_k)\n","        mask: torch.BoolTensor shape (N, M)\n","        where mask[i, j] = 1 means K[j] was masked for Q[i]\n","\n","    Return:\n","        scaled-dot attention: torch.Tensor shape (N, n_head, d_k)\n","    \"\"\"\n","    N, n_head, d_k = Q.shape\n","    attentions = []\n","\n","    for i in range(n_head):\n","        queries = Q[:, i, :]  # (N, d_k)\n","        keys = K[:, i, :]  # (M, d_k)\n","        values = V[:, i, :]  # (M, d_v)\n","        attentions.append(slow_queries_attention(queries, keys, values, mask)) # Apply the same mask for all heads\n","\n","    attentions = torch.stack(attentions)  # (n_head, N, d_v)\n","    attentions = torch.transpose(attentions, 0, 1)  # (N, n_head, d_v)\n","\n","    return attentions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dv6vlVraFyAy"},"source":["def heads_attention(Q, K, V, mask):\n","    \"\"\"Attentionn on many queries and many heads\n","    Arguments:\n","        Q: torch.Tensor shape (N, n_head, d_k)\n","        K: torch.Tensor shape (M, n_head, d_k)\n","        V: torch.Tensor shape (M, n_head, d_v)\n","        mask: torch.Tensor shape (N, M)\n","        where mask[i, j] = True means K[j] was masked for Q[i]\n","        \n","    Return:\n","        scaled-dot attention: torch.Tensor shape (N, n_head, d_v)\n","    \"\"\"\n","    N, n_head, d_k = Q.shape \n","    M, n_head, d_k = K.shape \n","    ########### YOUR CODE HERE #################\n","    \n","    ###########################################\n","    return attentions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7ZbOqiOFyA0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"788a1e10-9ec7-4d04-9321-b475624c0251"},"source":["Q = torch.rand(N_queries, n_head, d_k)\n","K = torch.rand(M_keys, n_head, d_k)\n","V = torch.rand(M_keys, n_head, d_v)\n","mask = torch.randint(low=0, high=2, size=(N_queries, M_keys)).bool()\n","\n","slow_attn = slow_heads_attention(Q, K, V, mask)\n","parl_attn = heads_attention(Q, K, V, mask)\n","\n","assert parl_attn.shape == slow_attn.shape\n","assert torch.allclose(parl_attn, slow_attn)\n","\n","def a2_generator():\n","    while True:\n","        Q = torch.rand(N_queries, n_head, d_k)\n","        K = torch.rand(M_keys, n_head, d_k)\n","        V = torch.rand(M_keys, n_head, d_v)\n","        mask = torch.randint(low=0, high=2, size=(N_queries, M_keys))\n","        \n","        yield (Q, K, V, mask)\n","        \n","generator = a2_generator()\n","benchmark(generator, slow_heads_attention, heads_attention)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Your outputs is correct\n","Timing of slow function:  AVG: 0.013750030994415283+-0.0009599313956945842\n","Timing of fast function:  AVG: 0.0003452897071838379+-5.010303190142049e-05\n","Speedup: 39.82172276885897 times\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iUe9gvz0FyA1"},"source":["## 3. Batch broadcasting\n","\n","In the previous exercise, you completed Multi-head Attention with a sequence! To make the most of parallel computing, in this exercise, you will build Multi-head Attention using broadcasting with multiple pairs of (Q, K, V) inputs in a batch.\n","\n","You need to complete the `multi_head_attention` function using broadcasting, the output should be the same as the output from `slow_multi_head_attention`."]},{"cell_type":"code","metadata":{"id":"uSFqHS8NFyA2"},"source":["def slow_multi_head_attention(Q, K, V, mask):\n","    \"\"\"Multi-head attention on a batch of Q, K, V\n","    Arguments:\n","        Q: torch.Tensor shape (B, N, n_head, d_k)\n","        K: torch.Tensor shape (B, M, n_head, d_k)\n","        V: torch.Tensor shape (B, M, n_head, d_v)\n","        mask: torch.BoolTensor shape (B, N, M)\n","        where mask[i] is `mask` for attention of record i: (Q[i], K[i], V[i])\n","\n","    Return:\n","        scaled-dot attention: torch.Tensor shape (B, N, n_head, d_v)\n","    \"\"\"\n","    B, N, n_head, d_k = Q.shape\n","\n","    attentions = []\n","    for single_Q, single_K, single_V, single_mask in zip(Q, K, V, mask):\n","        # single_Q, single_K: (N, n_head, d_k)\n","        # single_V: (N, n_head, d_v)\n","        # single_mask: (N, M)\n","        attention = slow_heads_attention(single_Q, single_K, single_V, single_mask)\n","        attentions.append(attention)\n","\n","    attentions = torch.stack(attentions)  # (B, N, n_head, d_v)\n","    return attentions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1s-82PPmFyA3"},"source":["def multi_head_attention(Q, K, V, mask):\n","    \"\"\"Multi-head attention on a batch of Q, K, V\n","    Arguments:\n","        Q: torch.Tensor shape (B, N, n_head, d_k)\n","        K: torch.Tensor shape (B, M, n_head, d_k)\n","        V: torch.Tensor shape (B, M, n_head, d_v)\n","        mask: torch.BoolTensor shape (B, N, M)\n","        where mask[i] is `mask` for attention of record i: (Q[i], K[i], V[i])\n","\n","    Return:\n","        scaled-dot attention: torch.Tensor shape (B, N, n_head d_v)\n","    \"\"\"\n","    B, N, n_head, d_k = Q.shape\n","    ########### YOUR CODE HERE #################\n","    \n","    ###########################################\n","    return attentions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GxJpoD4bFyA4","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e995e214-7e0e-4168-b153-5237a8cc673b"},"source":["Q = torch.rand(batch_size, N_queries, n_head, d_k)\n","K = torch.rand(batch_size, M_keys, n_head, d_k)\n","V = torch.rand(batch_size, M_keys, n_head, d_v)\n","mask = torch.randint(low=0, high=2, size=(batch_size, N_queries, M_keys)).bool()\n","\n","slow_attn = slow_multi_head_attention(Q, K, V, mask)\n","parl_attn = multi_head_attention(Q, K, V, mask)\n","\n","assert parl_attn.shape == slow_attn.shape\n","assert torch.allclose(slow_attn, parl_attn)\n","\n","def a3_generator():\n","    while True:\n","        Q = torch.rand(batch_size, N_queries, n_head, d_k)\n","        K = torch.rand(batch_size, M_keys, n_head, d_k)\n","        V = torch.rand(batch_size, M_keys, n_head, d_v)\n","        mask = torch.randint(low=0, high=2, size=(batch_size, N_queries, M_keys))\n","        yield (Q, K, V, mask)\n","\n","generator = a3_generator() # it's gonna take a while ...\n","benchmark(generator, slow_multi_head_attention, multi_head_attention)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Your outputs is correct\n","Timing of slow function:  AVG: 0.43443290710449217+-0.022188473290526433\n","Timing of fast function:  AVG: 0.008474128246307373+-0.0004484408840083568\n","Speedup: 51.26579330372981 times\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"OnIgeXkqFyA4"},"source":["## 4. `torch.einsum` and `einops.rearrange`\n","\n","The calculations (e.g., `transpose`, `matmul`, `stack`, `view` ...) on tensors are often not explicitly written (imagine doing the exercises above without comments). In this exercise, you will familiarize yourself with einops's `rearrange` and Pytorch's `einsum` both using Einstein summation(https://en.wikipedia.org/wiki/Einstein_notation): calculations on tensor will be expressed represented by a string (names of axes) of input and output tensor(s). This exercise, besides introducing `torch.einsum`, also introduces the most popular function in `einops`, `einops.rearrange`.\n","\n","Read more:\n","- Highly recommend: [Mat Kelcey : An illustrative einsum example](https://www.youtube.com/watch?v=SOaYrnQtd9g)\n","- [Writing a better code with pytorch and einops](http://einops.rocks/pytorch-examples.html)\n","\n","`einops.rearrange` and `torch.einsum` have some differences: `einops` supports axes names that are **string** from multiple contiguous characters (maybe `d_k`, `n_head`) while in `torch.einsum` are **1 lower case letters** (`h` is equivalent to `n_head`) corresponding to 1 axis of the tensor.\n","\n","`heads_attention` in Section 2 can be done with `einsum` & `rearrange` as follows:"]},{"cell_type":"code","metadata":{"id":"20aIg7OKFyA5","colab":{"base_uri":"https://localhost:8080/"},"outputId":"390b4bb4-4b76-4a3a-b277-bc3e13764cd8"},"source":["def heads_attention_with_einops1(Q, K, V, mask):\n","    \"\"\"Attentionn on a many queries and many heads\n","    More explicit, since we also introduce rearrange\n","    \n","    Arguments:\n","        Q: torch.Tensor shape (N, n_head, d_k)\n","        K: torch.Tensor shape (M, n_head, d_k)\n","        V: torch.Tensor shape (M, n_head, d_v)\n","        mask: torch.Tensor shape (N, M)\n","        where mask[i, j] = True means K[j] was masked for Q[i]\n","        \n","    Return:\n","        scaled-dot attention: torch.Tensor shape (N, n_head, d_v)\n","    \"\"\"\n","    N, n_head, d_k = Q.shape\n","    \n","    # Similar with reshape/view, but more expressive\n","    Q = rearrange(Q, \"N n_head d_k -> n_head N d_k\")\n","    K = rearrange(K, \"M n_head d_k -> n_head M d_k\")\n","    V = rearrange(V, \"M n_head d_k -> n_head M d_k\")\n","    \n","    similar_score = torch.einsum('hnd,hmd->hnm', Q, K) / (d_k ** 0.5) # Keep dimension h, reduce on `d`\n","    similar_score = similar_score.masked_fill(mask, value=float(\"-inf\"))\n","    \n","    attention_weight = F.softmax(similar_score, dim=-1)\n","    attentions = torch.einsum('hnm,hmd->hnd', attention_weight, V)\n","\n","    # We do `transpose` without comment the shape\n","    attentions = rearrange(attentions, 'n_head N d_v -> N n_head d_v')\n","    return attentions\n","\n","generator = a2_generator()\n","benchmark(generator, slow_heads_attention, heads_attention_with_einops1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Your outputs is correct\n","Timing of slow function:  AVG: 0.0136411452293396+-0.0009147664697613982\n","Timing of fast function:  AVG: 0.0005348515510559082+-0.0003580011645660165\n","Speedup: 25.504544583275756 times\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2ey9gRTTFyA5"},"source":["In the example above:\n","- `Q = rearrange(Q, \"N n_head d_k -> n_head N d_k\")` is equivalent to `Q.transpose(0, 1)`.\n","- String `hnd,hmd->hnm` describes calculation with 2 tensors (in our case, they are `Q` và `K`).\n","- `Q` has the shape `(n_head N d_k)`, abbreviated as `hnd`\n","- `K` has the shape `(n_head M d_k)`, abbreviated as `hmd`\n","- `similar_score` is calculated by doing dot-product on the `d_k` axis, resulting in a tensor of shape `(n_head N M)` abbreviated as `hnm` and located to the right of the `->`\n","\n","You can also use `einsum` completely as follows (if you have already mastered `einsum`)"]},{"cell_type":"code","metadata":{"id":"6TlUFIpTFyA6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"063006bf-f710-4737-ec46-96f509d7f6e7"},"source":["def heads_attention_with_einops2(Q, K, V, mask):\n","    \"\"\"Attentionn on a many queries and many heads\n","    Arguments:\n","        Q: torch.Tensor shape (N, n_head, d_k)\n","        K: torch.Tensor shape (M, n_head, d_k)\n","        V: torch.Tensor shape (M, n_head, d_v)\n","        mask: torch.Tensor shape (N, M)\n","        where mask[i, j] = True means K[j] was masked for Q[i]\n","        \n","    Return:\n","        scaled-dot attention: torch.Tensor shape (N, n_head, d_v)\n","    \"\"\"\n","    N, n_head, d_k = Q.shape\n","    similar_score = torch.einsum('nhd,mhd->hnm', Q, K) / (d_k ** 0.5)\n","    similar_score = similar_score.masked_fill(mask, value=float(\"-inf\"))\n","    attention_weight = F.softmax(similar_score, dim=-1)\n","    attentions = torch.einsum('hnm, mhd->nhd', attention_weight, V)\n","    return attentions\n","\n","generator = a2_generator()\n","benchmark(generator, slow_heads_attention, heads_attention_with_einops2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Your outputs is correct\n","Timing of slow function:  AVG: 0.013489282131195069+-0.0006958524786590557\n","Timing of fast function:  AVG: 0.0003887104988098144+-4.368779148447676e-05\n","Speedup: 34.70264418506229 times\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VhEj5O7ZFyA6"},"source":["The speed of `einsum` and `rearrange` is on par with broadcasting implementation.\n","\n","Now it's your turn to complete the `multi_head_attention_einops` function using `einsum` and `rearrange` like the example above!\n","\n","Hint: Just adding 1 dimension `b` in einops is done!"]},{"cell_type":"code","metadata":{"id":"Snewd4hQFyA7"},"source":["def multi_head_attention_einops(Q, K, V, mask):\n","    \"\"\"Multi-head attention on a batch of Q, K, V\n","    Arguments:\n","        Q: torch.Tensor shape (B, N, n_head, d_k)\n","        K: torch.Tensor shape (B, M, n_head, d_k)\n","        V: torch.Tensor shape (B, M, n_head, d_v)\n","        mask: torch.BoolTensor shape (B, N, M)\n","        where mask[i] is `mask` for attention of record i: (Q[i], K[i], V[i])\n","        \n","    Return:\n","        scaled-dot attention: torch.Tensor shape (B, N, n_head d_v)\n","    \"\"\"\n","    B, N, n_head, d_k = Q.shape\n","    ########### YOUR CODE HERE #################\n","    \n","    ###########################################\n","    return attentions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7z1vseegFyA7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f9efea56-b8c5-42bc-95aa-916a72a1f1bc"},"source":["generator = a3_generator()\n","benchmark(generator, slow_multi_head_attention, multi_head_attention_einops)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Your outputs is correct\n","Timing of slow function:  AVG: 0.4410352683067322+-0.03237853759710465\n","Timing of fast function:  AVG: 0.008850009441375732+-0.00023571950844938092\n","Speedup: 49.83444042949782 times\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hE0PLx2xFyBG"},"source":["Well done! Keep up the good work!"]},{"cell_type":"markdown","metadata":{"id":"cvMjvj-iX-vN"},"source":["## References\n","\n","- [Thomas Viehmann - visualize-jit-models](https://github.com/t-vi/pytorch-tvmisc/blob/master/hacks/visualize-jit-models.ipynb)\n","- [Sasha Rush - The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\n","- [Andrej Karpathy - min-gpt](https://github.com/karpathy/minGPT/tree/master/mingpt)"]}]}