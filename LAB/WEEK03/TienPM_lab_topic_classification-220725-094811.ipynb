{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Tutorial] topic_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TOPIC CLASSIFICATION WITH TRANSFORMERS\n",
        "*VietAI Advanced NLP*"
      ],
      "metadata": {
        "id": "x2EkNt6ixUE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction"
      ],
      "metadata": {
        "id": "e7ok-2jXau-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In this lab, VietAI introduces a tutorial on how to use huggingface's transformers library to deal with topic classification task.*"
      ],
      "metadata": {
        "id": "q7wOFpOHW6cV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic classification** is an NLP task in which we need to classify a document/text to a specific topic.\n",
        "\n",
        "This notebook shows you how to deal with Vietnam news topics classification using transformers library. After this you will know:\n",
        "\n",
        "\n",
        "1.   Simple preprocessing for Vietnamese text\n",
        "2.   Using some components in *transformers*:\n",
        "\n",
        "      *   Datasets\n",
        "      *   Tokenizer\n",
        "      *   Trainer\n",
        "      *   Pretrained language model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c40jbZeoaizG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find some of the resources used in this tutorial from:\n",
        "\n",
        "\n",
        "*   dataset: https://github.com/duyvuleo/VNTC/tree/master/Data/10Topics/Ver1.1\n",
        "*   underthesea: https://underthesea.readthedocs.io/en/latest/readme.html\n",
        "*   references: \n",
        "\n",
        "> https://huggingface.co/docs/transformers/training\n",
        "\n",
        "> https://huggingface.co/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6idwr64Vc4I1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Prepare your dataset"
      ],
      "metadata": {
        "id": "soF-MPlQZyia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenient you can download the data from my drive [here](https://drive.google.com/drive/folders/1uqXk9qiXqJ7xd8P3khvlOawdi2tzBTem?usp=sharing)"
      ],
      "metadata": {
        "id": "z9oUQglP3XWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Let's take a look at the dataset.*"
      ],
      "metadata": {
        "id": "pKAAKmrGigbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 10 topics:\n",
        "\n",
        "*   Chinh tri Xa hoi\n",
        "*   Doi song\n",
        "*   Khoa hoc\n",
        "*   Kinh doanh\n",
        "*   Phap luat\n",
        "*   Suc khoe\n",
        "*   The gioi\n",
        "*   The thao\n",
        "*   Van hoa\n",
        "*   Vi tinh\n",
        "\n",
        "\n",
        "\n",
        "Example of a document\n",
        "\n",
        "```\n",
        "Táº¡o bá»™ kit phÃ¡t hiá»‡n vi khuáº©n bá»‡nh thÆ°Æ¡ng hÃ n (NLÄ)- Tháº¡c sÄ© Pháº¡m ThÃ¡i BÃ¬nh, Trung tÃ¢m PhÃ¡t triá»ƒn khoa há»c vÃ  cÃ´ng nghá»‡ tráº», vá»«a nghiÃªn cá»©u cháº¿ táº¡o thÃ nh cÃ´ng bá»™ kit phÃ¡t hiá»‡n vi khuáº©n bá»‡nh thÆ°Æ¡ng hÃ n Salmonella typhi.\n",
        "Dá»±a trÃªn nguyÃªn táº¯c khÃ¡ng thá»ƒ Ä‘áº·c hiá»‡u cá»§a vi khuáº©n thÆ°Æ¡ng hÃ n Salmonella typhi cÃ³ trong huyáº¿t thanh ngÆ°á»i, náº¿u huyáº¿t thanh cá»§a ngÆ°á»i cÃ³ chá»©a vi khuáº©n bá»‡nh thÆ°Æ¡ng hÃ n thÃ¬ men gáº¯n trÃªn bá»™ kit sáº½ chuyá»ƒn tá»« khÃ´ng mÃ u sang cÃ³ mÃ u (xanh hoáº·c vÃ ng) \n",
        "Vá»›i bá»™ kit ELISA, chá»‰ cáº§n láº¥y cá»§a bá»‡nh nhÃ¢n khoáº£ng 1cc mÃ¡u, á»§ trong 2 giá», cÃ³ thá»ƒ phÃ¡t hiá»‡n bá»‡nh thÆ°Æ¡ng hÃ n chÃ­nh xÃ¡c Ä‘áº¿n 98%. Bá»™ kit nÃ y Ä‘Ã£ Ä‘Æ°á»£c dÃ¹ng trÃªn 100 máº«u bá»‡nh pháº©m táº¡i Bá»‡nh viá»‡n Äa khoa Trung tÃ¢m An Giang, TP Long XuyÃªn, tá»‰nh An Giang. Theo chá»§ nhiá»‡m Ä‘á» tÃ i, náº¿u bá»™ kit nÃ y Ä‘Æ°á»£c sáº£n xuáº¥t táº¡i Viá»‡t Nam, giÃ¡ thÃ nh sáº½ giáº£m hÆ¡n 50% so vá»›i giÃ¡ nháº­p kháº©u tá»« Má»¹. Äá» tÃ i Ä‘Ã£ Ä‘Æ°á»£c há»™i Ä‘á»“ng nghiá»‡m thu do Sá»Ÿ Khoa há»c vÃ  CÃ´ng nghá»‡ TPHCM láº­p Ä‘Ã¡nh giÃ¡ vÃ o loáº¡i xuáº¥t sáº¯c. CÃ´ng ty TNHH SX-TM-DV Nam Khoa (Q.7 â€“ TPHCM) cÅ©ng vá»«a Ä‘áº§u tÆ° cho nhÃ³m nghiÃªn cá»©u Ä‘á» tÃ i 100 triá»‡u Ä‘á»“ng Ä‘á»ƒ Ä‘Æ°a vÃ o sáº£n xuáº¥t thá»­ (áº£nh). \n",
        "```\n",
        "\n",
        "> Label: Khoa hoc\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UTC_QPaAa6mU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, a single document contains several lines and it is not tokenized. Since a pretrained language model learns the context, we do not try to remove stopwords or numbers like some traditional techniques. Therefore, we only need a few steps:\n",
        "\n",
        "*   Combine all lines in a document\n",
        "*   Apply word tokenization"
      ],
      "metadata": {
        "id": "lMqOvNpydtXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea"
      ],
      "metadata": {
        "id": "WuYXI_MSppdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from underthesea import word_tokenize\n",
        "import os\n",
        "\n",
        "def clean_document(document):\n",
        "    \"\"\"\n",
        "        return cleaned document (str)\n",
        "          - Combine lines\n",
        "          - Word tokenize using underthesea library\n",
        "    \"\"\"\n",
        "    # combine lines: replace the newline symbols with space\n",
        "    combine_lines = document.replace(\"\\n\", \" \") \n",
        "\n",
        "    # optional step: fix the whitespace between words (maybe duplicated)\n",
        "    whitespace_fix = \" \".join(combine_lines.split())\n",
        "\n",
        "    # word tokenize\n",
        "    tokenized_doc = word_tokenize(whitespace_fix)\n",
        "    tokenized_doc = \" \".join([w.strip().replace(\" \", \"_\") for w in tokenized_doc])\n",
        "    return tokenized_doc\n",
        "\n",
        "def process_data(datasrc, label2id, prefix):\n",
        "    \"\"\"\n",
        "    return a dictionary with elements composed of 2 keys:\n",
        "      - text: preprocessed text\n",
        "      - labels: id coresspond to label\n",
        "    \"\"\"\n",
        "    processed_data = {}\n",
        "\n",
        "    dp_id = 0\n",
        "    for label in os.listdir(datasrc):\n",
        "        label_id = label2id[label]\n",
        "        for filename in os.listdir(os.path.join(datasrc, label)):\n",
        "            try:\n",
        "                with open(os.path.join(datasrc, label, filename), \"r\", encoding= \"utf-16\") as f:\n",
        "                    document = f.read()\n",
        "                    cleaned_doc = clean_document(document)\n",
        "\n",
        "                    id = prefix + str(dp_id)\n",
        "                    processed_data[id] = {}\n",
        "                    processed_data[id][\"text\"] = cleaned_doc\n",
        "                    processed_data[id][\"labels\"] = label_id\n",
        "                dp_id += 1\n",
        "            except:\n",
        "                print(os.path.join(label,filename))\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "\n",
        "def save_data(data, type):\n",
        "    # save your preprocessed data\n",
        "    with open(os.path.join(\"data/clean/\", type + \".json\"), \"w\") as f:\n",
        "        json.dump(data, f, indent= 4)\n",
        "    return\n"
      ],
      "metadata": {
        "id": "_46pIrZzXmGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# map topic to label id\n",
        "datasrc = \"data/raw/train/\"\n",
        "\n",
        "label2id = dict()\n",
        "\n",
        "for label_id, label in enumerate(os.listdir(datasrc)):\n",
        "    label2id[label] = label_id\n",
        "\n",
        "# save the label dict to use later\n",
        "with open(\"data/clean/label2id.json\", \"w\") as f:\n",
        "    json.dump(label2id, f, indent= 4)"
      ],
      "metadata": {
        "id": "NXvRjmwPfo_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process a large corpus may take a lot of time. Please wait.\n",
        "\n",
        "train_data = process_data(\"data/raw/train\", label2id, \"train\")\n",
        "# Let's make a validation set from trainset\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_keys = list(train_data.keys())\n",
        "train_ids, valid_ids = train_test_split(train_keys, test_size= 0.1)\n",
        "\n",
        "valid_data = {id:train_data[id] for id in valid_ids}\n",
        "train_data = {id:train_data[id] for id in train_ids}\n",
        "\n",
        "test_data = process_data(\"data/raw/test\", label2id, \"test\")"
      ],
      "metadata": {
        "id": "bmhEIoYFhXXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at a processed data sample\n",
        "print(json.dumps(train_data[\"train1\"], indent= 4, ensure_ascii= False))"
      ],
      "metadata": {
        "id": "dfMBfBcZSj9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save preprocess data\n",
        "save_data(train_data, \"train\")\n",
        "save_data(valid_data, \"valid\")\n",
        "save_data(test_data, \"test\")"
      ],
      "metadata": {
        "id": "S5CF7kdQ8s8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The testset in this corpus is quite large (even bigger than the trainset). So I sample a small amount of testset to reduce testing time and processing time before apply pretrained language model."
      ],
      "metadata": {
        "id": "Sbty7JTYlLWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"data/clean/test.json\", \"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "test_keys = list(test_data.keys())\n",
        "test, sample_test_ids = train_test_split(test_keys, test_size= 3000)\n",
        "print(len(sample_test_ids))\n",
        "\n",
        "sample_test = {id:test_data[id] for id in sample_test_ids}\n",
        "save_data(sample_test, \"sample_test\")"
      ],
      "metadata": {
        "id": "lMV9ATFhY6u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize class distribution in training set\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NpR1CL43ek75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training model with transformers"
      ],
      "metadata": {
        "id": "dgMn4ALzn1ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "!pip install datasets transformers"
      ],
      "metadata": {
        "id": "QrXvFsUtpsVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Loading dataset"
      ],
      "metadata": {
        "id": "Oyl6uRf4mKn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load your dataset from file or download from url. It is suggested to write a loading script.\n",
        "\n",
        "The script helps you load your dataset and return a DatasetDict object which is convenient to use some functions provided by huggingface (such as apply tokenizer).\n",
        "\n",
        "You can find an example of a script in _topic_data.py_. It contains 3 main functions:\n",
        "*   ```_info()```: which is in charge of specifying the dataset metadata as a datasets. DatasetInfo dataclass and in particular the datasets.Features which defines the names and types of each column in the dataset,\n",
        "\n",
        "*   ```_split_generator()```: which is in charge of downloading or retrieving the data files, organizing them by splits and defining specific arguments for the generation process if needed,\n",
        "\n",
        "*   ```_generate_examples()```: which is in charge of loading the files for a split and yielding examples with the format specified in the features.\n",
        "\n",
        "For more details: https://huggingface.co/docs/datasets/dataset_script\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_x6FhLo_mrM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "cleaned_data = load_dataset(\"topic_data.py\")"
      ],
      "metadata": {
        "id": "K9FLsk6AgsMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cleaned_data)"
      ],
      "metadata": {
        "id": "iK1eZncEhAiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Tokenizer"
      ],
      "metadata": {
        "id": "mhLYZnbFmQ5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use pretrained language model such as BERT, GPT, Roberta... \n",
        "\n",
        "We need to tokenize words using the strategy in these models (BPE, wordpiece, ...)\n"
      ],
      "metadata": {
        "id": "lLHjrbgTqCI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huggingface allows us to get the tokenizer corresponding to the model by the model name on their [hub](https://huggingface.co/models). In this notebook, we use PhoBERT-base (vinai/phobert-base).\n",
        "\n",
        "Apply tokenizer to the dataset by  ```map()``` function. This will return a dataset with additional elements which is required in function ```forward``` of the model."
      ],
      "metadata": {
        "id": "Fhk-z5_ks7mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")# model name in huggingface's hub\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = cleaned_data.map(tokenize_function, batched=True) # Apply tokenizer to the dataset"
      ],
      "metadata": {
        "id": "BYImvCYynSpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "additional elements: 'input_ids', 'token_type_ids', 'attention_mask'\n",
        "\"\"\"\n",
        "\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "udqV1YRWWZMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Pretrained language model"
      ],
      "metadata": {
        "id": "qo0Q_HYamUjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we use [PhoBERT](https://arxiv.org/abs/2003.00744) which is a popular pretrained language model for Vietnamese. It's architecture is similar to BERT.\n",
        "\n",
        "For finetuning on the classification task, we use ```ModelForSequenceClassification``` object to automatically add a linear layer (classifier) on the top of the pretrained model."
      ],
      "metadata": {
        "id": "yv48CwN5u98m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=len(label2id))"
      ],
      "metadata": {
        "id": "MmiAaWYGoE_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Trainer"
      ],
      "metadata": {
        "id": "j17H0GtXmbZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer object helps us to train, evaluate the model on the dataset. The model and tokenized data are input parameters of this object.\n",
        "\n",
        "It is also required a TrainingArguments object to define hyper-parameters such as:\n",
        "\n",
        "\n",
        "*   num_train_epochs\n",
        "*   batch_size\n",
        "*   learning_rate\n",
        "*   ...\n",
        "\n",
        "For more details: [Huggingface Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "65D61KsAwHM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "# define metric for evaluation (accuracy)\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "zPgGCjAVocV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 8\n",
        "lr = 2e-5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"topic_classification_result\",\n",
        "    evaluation_strategy = \"steps\", #print evaluation after finishing an epoch\n",
        "    num_train_epochs=epochs,\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    save_total_limit=1,\n",
        "    save_steps=2000,\n",
        "    eval_steps=2000,\n",
        "    gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "DjZ_JPZGRDrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's evaluate on our test set\n",
        "test_eval = trainer.predict(tokenized_datasets[\"test\"])\n",
        "print(json.dumps(test_eval.metrics, indent= 4))"
      ],
      "metadata": {
        "id": "vPvNaFOMwvIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model. The model will be saved in the output_dir defined in the training arguments\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "0AyjBLfowjpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "with open(\"data/clean/label2id.json\" ,\"r\") as f:\n",
        "    label2id = json.load(f)\n",
        "id2label = {value: key for key, value in label2id.items()}\n",
        "label_tags = [id2label[i] for i in range(10)]\n",
        "\n",
        "predictions = []\n",
        "for pred in test_eval.predictions:\n",
        "    p = np.argmax(pred)\n",
        "    predictions.append(p)\n",
        "\n",
        "cfs = confusion_matrix(tokenized_datasets[\"test\"]['labels'], predictions)\n",
        "fig, ax1 = plt.subplots(1,1, figsize=(10,8))\n",
        "sns.heatmap(cfs, annot=True, fmt='d',\n",
        "            xticklabels=label_tags, yticklabels=label_tags, ax= ax1)"
      ],
      "metadata": {
        "id": "bh18u5mV3fOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom loss function"
      ],
      "metadata": {
        "id": "mfD9igc_jmjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to custom a loss function. Use the above information of class distribution to define WeightedCrossEntropyLoss to deal with imbalanced data. For instance, the weight can be calculated as:\n",
        "\n",
        "$w_i = log(\\frac{N}{N_i})$ \n",
        "\n",
        "where $N$ is number of data points and $N_i$ is the number of data points in class $i^th$"
      ],
      "metadata": {
        "id": "WsuWwHyPGu-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained language model is sometimes claimed that is less sensitve to the unbalanced data. Can the result proves that?"
      ],
      "metadata": {
        "id": "kQwqBkBh0Tc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 8\n",
        "lr = 2e-5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"topic_classification_result\",\n",
        "    evaluation_strategy = \"steps\", #print evaluation after finishing an epoch\n",
        "    num_train_epochs=epochs,\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    save_total_limit=1,\n",
        "    save_steps=10000,\n",
        "    eval_steps=1000,\n",
        "    gradient_accumulation_steps=2,\n",
        ")"
      ],
      "metadata": {
        "id": "ZDfYe6rNSl-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom a trainer to define the loss function\n",
        "# hint: read the doc https://huggingface.co/docs/transformers/main_classes/trainer\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "#####################\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "Ce8vCpq1BrRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "GjhYTH1yV5DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eval = trainer.predict(tokenized_datasets[\"test\"])\n",
        "print(json.dumps(test_eval.metrics, indent= 4))"
      ],
      "metadata": {
        "id": "o2gGs8k-lScz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Demo"
      ],
      "metadata": {
        "id": "SwHI1CvKxzpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load our fine-tuned model and make a simple demo ðŸ¤—"
      ],
      "metadata": {
        "id": "eM7uhjD7xNLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"topic_classification_result\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
      ],
      "metadata": {
        "id": "bQU8W7jox4XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline #pipeline for text classfication\n",
        "\n",
        "# load label2id\n",
        "with open(\"data/clean/label2id.json\" ,\"r\") as f:\n",
        "    label2id = json.load(f)\n",
        "id2label = {value: key for key, value in label2id.items()}\n",
        "\n",
        "while True:\n",
        "    document = input(\"Input text here: \")\n",
        "    if document == \"exit\":\n",
        "        break\n",
        "    document = clean_document(document)\n",
        "    pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
        "    pred_topic_id = model.config.label2id[pipe(document)[0]['label']]\n",
        "    print(\"Topic: \", id2label[pred_topic_id])\n",
        "    print(\"=\"*100)"
      ],
      "metadata": {
        "id": "n53UZZ0dyaqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}