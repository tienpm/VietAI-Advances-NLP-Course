{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Tutorial] topic_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TOPIC CLASSIFICATION WITH TRANSFORMERS\n",
        "*VietAI Advanced NLP*"
      ],
      "metadata": {
        "id": "x2EkNt6ixUE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Introduction"
      ],
      "metadata": {
        "id": "e7ok-2jXau-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*In this lab, VietAI introduces a tutorial on how to use huggingface's transformers library to deal with topic classification task.*"
      ],
      "metadata": {
        "id": "q7wOFpOHW6cV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Topic classification** is an NLP task in which we need to classify a document/text to a specific topic.\n",
        "\n",
        "This notebook shows you how to deal with Vietnam news topics classification using transformers library. After this you will know:\n",
        "\n",
        "\n",
        "1.   Simple preprocessing for Vietnamese text\n",
        "2.   Using some components in *transformers*:\n",
        "\n",
        "      *   Datasets\n",
        "      *   Tokenizer\n",
        "      *   Trainer\n",
        "      *   Pretrained language model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c40jbZeoaizG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find some of the resources used in this tutorial from:\n",
        "\n",
        "\n",
        "*   dataset: https://github.com/duyvuleo/VNTC/tree/master/Data/10Topics/Ver1.1\n",
        "*   underthesea: https://underthesea.readthedocs.io/en/latest/readme.html\n",
        "*   references: \n",
        "\n",
        "> https://huggingface.co/docs/transformers/training\n",
        "\n",
        "> https://huggingface.co/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6idwr64Vc4I1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Prepare your dataset"
      ],
      "metadata": {
        "id": "soF-MPlQZyia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenient you can download the data from my drive [here](https://drive.google.com/drive/folders/1uqXk9qiXqJ7xd8P3khvlOawdi2tzBTem?usp=sharing)"
      ],
      "metadata": {
        "id": "z9oUQglP3XWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Let's take a look at the dataset.*"
      ],
      "metadata": {
        "id": "pKAAKmrGigbO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 10 topics:\n",
        "\n",
        "*   Chinh tri Xa hoi\n",
        "*   Doi song\n",
        "*   Khoa hoc\n",
        "*   Kinh doanh\n",
        "*   Phap luat\n",
        "*   Suc khoe\n",
        "*   The gioi\n",
        "*   The thao\n",
        "*   Van hoa\n",
        "*   Vi tinh\n",
        "\n",
        "\n",
        "\n",
        "Example of a document\n",
        "\n",
        "```\n",
        "Tạo bộ kit phát hiện vi khuẩn bệnh thương hàn (NLĐ)- Thạc sĩ Phạm Thái Bình, Trung tâm Phát triển khoa học và công nghệ trẻ, vừa nghiên cứu chế tạo thành công bộ kit phát hiện vi khuẩn bệnh thương hàn Salmonella typhi.\n",
        "Dựa trên nguyên tắc kháng thể đặc hiệu của vi khuẩn thương hàn Salmonella typhi có trong huyết thanh người, nếu huyết thanh của người có chứa vi khuẩn bệnh thương hàn thì men gắn trên bộ kit sẽ chuyển từ không màu sang có màu (xanh hoặc vàng) \n",
        "Với bộ kit ELISA, chỉ cần lấy của bệnh nhân khoảng 1cc máu, ủ trong 2 giờ, có thể phát hiện bệnh thương hàn chính xác đến 98%. Bộ kit này đã được dùng trên 100 mẫu bệnh phẩm tại Bệnh viện Đa khoa Trung tâm An Giang, TP Long Xuyên, tỉnh An Giang. Theo chủ nhiệm đề tài, nếu bộ kit này được sản xuất tại Việt Nam, giá thành sẽ giảm hơn 50% so với giá nhập khẩu từ Mỹ. Đề tài đã được hội đồng nghiệm thu do Sở Khoa học và Công nghệ TPHCM lập đánh giá vào loại xuất sắc. Công ty TNHH SX-TM-DV Nam Khoa (Q.7 – TPHCM) cũng vừa đầu tư cho nhóm nghiên cứu đề tài 100 triệu đồng để đưa vào sản xuất thử (ảnh). \n",
        "```\n",
        "\n",
        "> Label: Khoa hoc\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UTC_QPaAa6mU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, a single document contains several lines and it is not tokenized. Since a pretrained language model learns the context, we do not try to remove stopwords or numbers like some traditional techniques. Therefore, we only need a few steps:\n",
        "\n",
        "*   Combine all lines in a document\n",
        "*   Apply word tokenization"
      ],
      "metadata": {
        "id": "lMqOvNpydtXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea"
      ],
      "metadata": {
        "id": "WuYXI_MSppdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from underthesea import word_tokenize\n",
        "import os\n",
        "\n",
        "def clean_document(document):\n",
        "    \"\"\"\n",
        "        return cleaned document (str)\n",
        "          - Combine lines\n",
        "          - Word tokenize using underthesea library\n",
        "    \"\"\"\n",
        "    # combine lines: replace the newline symbols with space\n",
        "    combine_lines = document.replace(\"\\n\", \" \") \n",
        "\n",
        "    # optional step: fix the whitespace between words (maybe duplicated)\n",
        "    whitespace_fix = \" \".join(combine_lines.split())\n",
        "\n",
        "    # word tokenize\n",
        "    tokenized_doc = word_tokenize(whitespace_fix)\n",
        "    tokenized_doc = \" \".join([w.strip().replace(\" \", \"_\") for w in tokenized_doc])\n",
        "    return tokenized_doc\n",
        "\n",
        "def process_data(datasrc, label2id, prefix):\n",
        "    \"\"\"\n",
        "    return a dictionary with elements composed of 2 keys:\n",
        "      - text: preprocessed text\n",
        "      - labels: id coresspond to label\n",
        "    \"\"\"\n",
        "    processed_data = {}\n",
        "\n",
        "    dp_id = 0\n",
        "    for label in os.listdir(datasrc):\n",
        "        label_id = label2id[label]\n",
        "        for filename in os.listdir(os.path.join(datasrc, label)):\n",
        "            try:\n",
        "                with open(os.path.join(datasrc, label, filename), \"r\", encoding= \"utf-16\") as f:\n",
        "                    document = f.read()\n",
        "                    cleaned_doc = clean_document(document)\n",
        "\n",
        "                    id = prefix + str(dp_id)\n",
        "                    processed_data[id] = {}\n",
        "                    processed_data[id][\"text\"] = cleaned_doc\n",
        "                    processed_data[id][\"labels\"] = label_id\n",
        "                dp_id += 1\n",
        "            except:\n",
        "                print(os.path.join(label,filename))\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "\n",
        "def save_data(data, type):\n",
        "    # save your preprocessed data\n",
        "    with open(os.path.join(\"data/clean/\", type + \".json\"), \"w\") as f:\n",
        "        json.dump(data, f, indent= 4)\n",
        "    return\n"
      ],
      "metadata": {
        "id": "_46pIrZzXmGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# map topic to label id\n",
        "datasrc = \"data/raw/train/\"\n",
        "\n",
        "label2id = dict()\n",
        "\n",
        "for label_id, label in enumerate(os.listdir(datasrc)):\n",
        "    label2id[label] = label_id\n",
        "\n",
        "# save the label dict to use later\n",
        "with open(\"data/clean/label2id.json\", \"w\") as f:\n",
        "    json.dump(label2id, f, indent= 4)"
      ],
      "metadata": {
        "id": "NXvRjmwPfo_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process a large corpus may take a lot of time. Please wait.\n",
        "\n",
        "train_data = process_data(\"data/raw/train\", label2id, \"train\")\n",
        "# Let's make a validation set from trainset\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_keys = list(train_data.keys())\n",
        "train_ids, valid_ids = train_test_split(train_keys, test_size= 0.1)\n",
        "\n",
        "valid_data = {id:train_data[id] for id in valid_ids}\n",
        "train_data = {id:train_data[id] for id in train_ids}\n",
        "\n",
        "test_data = process_data(\"data/raw/test\", label2id, \"test\")"
      ],
      "metadata": {
        "id": "bmhEIoYFhXXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at a processed data sample\n",
        "print(json.dumps(train_data[\"train1\"], indent= 4, ensure_ascii= False))"
      ],
      "metadata": {
        "id": "dfMBfBcZSj9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save preprocess data\n",
        "save_data(train_data, \"train\")\n",
        "save_data(valid_data, \"valid\")\n",
        "save_data(test_data, \"test\")"
      ],
      "metadata": {
        "id": "S5CF7kdQ8s8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The testset in this corpus is quite large (even bigger than the trainset). So I sample a small amount of testset to reduce testing time and processing time before apply pretrained language model."
      ],
      "metadata": {
        "id": "Sbty7JTYlLWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "with open(\"data/clean/test.json\", \"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "test_keys = list(test_data.keys())\n",
        "test, sample_test_ids = train_test_split(test_keys, test_size= 3000)\n",
        "print(len(sample_test_ids))\n",
        "\n",
        "sample_test = {id:test_data[id] for id in sample_test_ids}\n",
        "save_data(sample_test, \"sample_test\")"
      ],
      "metadata": {
        "id": "lMV9ATFhY6u7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize class distribution in training set\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NpR1CL43ek75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training model with transformers"
      ],
      "metadata": {
        "id": "dgMn4ALzn1ct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install required packages\n",
        "!pip install datasets transformers"
      ],
      "metadata": {
        "id": "QrXvFsUtpsVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Loading dataset"
      ],
      "metadata": {
        "id": "Oyl6uRf4mKn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load your dataset from file or download from url. It is suggested to write a loading script.\n",
        "\n",
        "The script helps you load your dataset and return a DatasetDict object which is convenient to use some functions provided by huggingface (such as apply tokenizer).\n",
        "\n",
        "You can find an example of a script in _topic_data.py_. It contains 3 main functions:\n",
        "*   ```_info()```: which is in charge of specifying the dataset metadata as a datasets. DatasetInfo dataclass and in particular the datasets.Features which defines the names and types of each column in the dataset,\n",
        "\n",
        "*   ```_split_generator()```: which is in charge of downloading or retrieving the data files, organizing them by splits and defining specific arguments for the generation process if needed,\n",
        "\n",
        "*   ```_generate_examples()```: which is in charge of loading the files for a split and yielding examples with the format specified in the features.\n",
        "\n",
        "For more details: https://huggingface.co/docs/datasets/dataset_script\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_x6FhLo_mrM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "cleaned_data = load_dataset(\"topic_data.py\")"
      ],
      "metadata": {
        "id": "K9FLsk6AgsMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cleaned_data)"
      ],
      "metadata": {
        "id": "iK1eZncEhAiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Tokenizer"
      ],
      "metadata": {
        "id": "mhLYZnbFmQ5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use pretrained language model such as BERT, GPT, Roberta... \n",
        "\n",
        "We need to tokenize words using the strategy in these models (BPE, wordpiece, ...)\n"
      ],
      "metadata": {
        "id": "lLHjrbgTqCI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Huggingface allows us to get the tokenizer corresponding to the model by the model name on their [hub](https://huggingface.co/models). In this notebook, we use PhoBERT-base (vinai/phobert-base).\n",
        "\n",
        "Apply tokenizer to the dataset by  ```map()``` function. This will return a dataset with additional elements which is required in function ```forward``` of the model."
      ],
      "metadata": {
        "id": "Fhk-z5_ks7mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")# model name in huggingface's hub\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "tokenized_datasets = cleaned_data.map(tokenize_function, batched=True) # Apply tokenizer to the dataset"
      ],
      "metadata": {
        "id": "BYImvCYynSpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "additional elements: 'input_ids', 'token_type_ids', 'attention_mask'\n",
        "\"\"\"\n",
        "\n",
        "print(tokenized_datasets)"
      ],
      "metadata": {
        "id": "udqV1YRWWZMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3. Pretrained language model"
      ],
      "metadata": {
        "id": "qo0Q_HYamUjR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we use [PhoBERT](https://arxiv.org/abs/2003.00744) which is a popular pretrained language model for Vietnamese. It's architecture is similar to BERT.\n",
        "\n",
        "For finetuning on the classification task, we use ```ModelForSequenceClassification``` object to automatically add a linear layer (classifier) on the top of the pretrained model."
      ],
      "metadata": {
        "id": "yv48CwN5u98m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"vinai/phobert-base\", num_labels=len(label2id))"
      ],
      "metadata": {
        "id": "MmiAaWYGoE_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4. Trainer"
      ],
      "metadata": {
        "id": "j17H0GtXmbZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trainer object helps us to train, evaluate the model on the dataset. The model and tokenized data are input parameters of this object.\n",
        "\n",
        "It is also required a TrainingArguments object to define hyper-parameters such as:\n",
        "\n",
        "\n",
        "*   num_train_epochs\n",
        "*   batch_size\n",
        "*   learning_rate\n",
        "*   ...\n",
        "\n",
        "For more details: [Huggingface Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "65D61KsAwHM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "# define metric for evaluation (accuracy)\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "zPgGCjAVocV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 8\n",
        "lr = 2e-5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"topic_classification_result\",\n",
        "    evaluation_strategy = \"steps\", #print evaluation after finishing an epoch\n",
        "    num_train_epochs=epochs,\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    save_total_limit=1,\n",
        "    save_steps=2000,\n",
        "    eval_steps=2000,\n",
        "    gradient_accumulation_steps=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "DjZ_JPZGRDrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's evaluate on our test set\n",
        "test_eval = trainer.predict(tokenized_datasets[\"test\"])\n",
        "print(json.dumps(test_eval.metrics, indent= 4))"
      ],
      "metadata": {
        "id": "vPvNaFOMwvIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the best model. The model will be saved in the output_dir defined in the training arguments\n",
        "trainer.save_model()"
      ],
      "metadata": {
        "id": "0AyjBLfowjpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "with open(\"data/clean/label2id.json\" ,\"r\") as f:\n",
        "    label2id = json.load(f)\n",
        "id2label = {value: key for key, value in label2id.items()}\n",
        "label_tags = [id2label[i] for i in range(10)]\n",
        "\n",
        "predictions = []\n",
        "for pred in test_eval.predictions:\n",
        "    p = np.argmax(pred)\n",
        "    predictions.append(p)\n",
        "\n",
        "cfs = confusion_matrix(tokenized_datasets[\"test\"]['labels'], predictions)\n",
        "fig, ax1 = plt.subplots(1,1, figsize=(10,8))\n",
        "sns.heatmap(cfs, annot=True, fmt='d',\n",
        "            xticklabels=label_tags, yticklabels=label_tags, ax= ax1)"
      ],
      "metadata": {
        "id": "bh18u5mV3fOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom loss function"
      ],
      "metadata": {
        "id": "mfD9igc_jmjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to custom a loss function. Use the above information of class distribution to define WeightedCrossEntropyLoss to deal with imbalanced data. For instance, the weight can be calculated as:\n",
        "\n",
        "$w_i = log(\\frac{N}{N_i})$ \n",
        "\n",
        "where $N$ is number of data points and $N_i$ is the number of data points in class $i^th$"
      ],
      "metadata": {
        "id": "WsuWwHyPGu-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretrained language model is sometimes claimed that is less sensitve to the unbalanced data. Can the result proves that?"
      ],
      "metadata": {
        "id": "kQwqBkBh0Tc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "epochs = 2\n",
        "batch_size = 8\n",
        "lr = 2e-5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"topic_classification_result\",\n",
        "    evaluation_strategy = \"steps\", #print evaluation after finishing an epoch\n",
        "    num_train_epochs=epochs,\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    save_total_limit=1,\n",
        "    save_steps=10000,\n",
        "    eval_steps=1000,\n",
        "    gradient_accumulation_steps=2,\n",
        ")"
      ],
      "metadata": {
        "id": "ZDfYe6rNSl-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom a trainer to define the loss function\n",
        "# hint: read the doc https://huggingface.co/docs/transformers/main_classes/trainer\n",
        "\n",
        "### YOUR CODE HERE ###\n",
        "\n",
        "\n",
        "#####################\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "Ce8vCpq1BrRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "GjhYTH1yV5DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_eval = trainer.predict(tokenized_datasets[\"test\"])\n",
        "print(json.dumps(test_eval.metrics, indent= 4))"
      ],
      "metadata": {
        "id": "o2gGs8k-lScz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Demo"
      ],
      "metadata": {
        "id": "SwHI1CvKxzpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load our fine-tuned model and make a simple demo 🤗"
      ],
      "metadata": {
        "id": "eM7uhjD7xNLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"topic_classification_result\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")"
      ],
      "metadata": {
        "id": "bQU8W7jox4XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextClassificationPipeline #pipeline for text classfication\n",
        "\n",
        "# load label2id\n",
        "with open(\"data/clean/label2id.json\" ,\"r\") as f:\n",
        "    label2id = json.load(f)\n",
        "id2label = {value: key for key, value in label2id.items()}\n",
        "\n",
        "while True:\n",
        "    document = input(\"Input text here: \")\n",
        "    if document == \"exit\":\n",
        "        break\n",
        "    document = clean_document(document)\n",
        "    pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer)\n",
        "    pred_topic_id = model.config.label2id[pipe(document)[0]['label']]\n",
        "    print(\"Topic: \", id2label[pred_topic_id])\n",
        "    print(\"=\"*100)"
      ],
      "metadata": {
        "id": "n53UZZ0dyaqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}